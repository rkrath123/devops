Security
=========
In the video, we had seen how the sample kubeconfig file was making use of certificates for authentication. 
Depending on the service provider, the provider can use any of the authentication modes, it can be certificates, tokens, etc.

In the older Digital Ocean based K8s cluster, the kubeconfig file used certificates. However, the newer one makes use of bearer tokens.

Replace the IP and Token in the below command based on your environment

curl -k https://IP-HERE:/api/v1/ --header "Authorization: Bearer $TOKEN" 


![image](https://user-images.githubusercontent.com/53966749/200806929-8ca11fa3-6935-4a7e-b505-1f193909642a.png)
![image](https://user-images.githubusercontent.com/53966749/200807083-e7439f5c-dd57-42b2-bf6b-48226ab0ed34.png)

Bearer token
=============
![image](https://user-images.githubusercontent.com/53966749/200813255-2ad4c798-f0ed-4f43-a870-26be5885e39d.png)

 Asymmetric Key Encryption
 ==========================
 ![image](https://user-images.githubusercontent.com/53966749/200815087-83ec9beb-9613-41c1-be71-defb0d32a2c5.png)
![image](https://user-images.githubusercontent.com/53966749/200815386-c4539d73-5a06-4443-bcb6-9a7ce9c49a42.png)
![image](https://user-images.githubusercontent.com/53966749/200815545-a8c3aa83-d387-4c2a-bc17-a4b7032ac730.png)
 ![image](https://user-images.githubusercontent.com/53966749/200815917-156f1c19-9a5e-49d9-9eb1-633eb4d42820.png)
![image](https://user-images.githubusercontent.com/53966749/200816216-f29e28be-070e-48b2-a82a-3b5a7fcc6dbd.png)
![image](https://user-images.githubusercontent.com/53966749/200816278-5ada4487-d8df-4de4-b82d-04dc3f253dcb.png)
![image](https://user-images.githubusercontent.com/53966749/200816436-99469c17-43c5-43c3-b0f7-2e5fe43a9d72.png)
![image](https://user-images.githubusercontent.com/53966749/200816968-23f96f9f-bc1b-4d89-8e69-4b7fcfde5021.png)
![image](https://user-images.githubusercontent.com/53966749/200817069-94298ac4-f5ad-407a-8048-cf8c56b27193.png)


Understanding SSLTLS
=======================
![image](https://user-images.githubusercontent.com/53966749/200817376-ce617ccc-bd6e-421a-abb2-ad74791957d0.png)
![image](https://user-images.githubusercontent.com/53966749/200817823-34703130-9c8f-4019-95e5-1afc994e627a.png)
![image](https://user-images.githubusercontent.com/53966749/200818115-c67e96ba-5580-4e70-b832-9a369dda538a.png)
![image](https://user-images.githubusercontent.com/53966749/200818332-80e35029-0faf-4bb1-9cc6-63fe46fc1760.png)
![image](https://user-images.githubusercontent.com/53966749/200819234-bdf9eea3-66d1-443d-8545-4478b4ccdf12.png)

Certhificate based authentication k8s
======================================

![image](https://user-images.githubusercontent.com/53966749/200819483-bac11423-96f9-40a8-8d0a-2c2b168c2d1f.png)
![image](https://user-images.githubusercontent.com/53966749/200819815-814ff3e3-6966-4846-b7b6-1660025e21cf.png)
![image](https://user-images.githubusercontent.com/53966749/200819876-2fba9fe1-c404-4ac4-9495-392d24683f97.png)

```
Pre-Requisite:
- Create a Linux container / Linux server were we can run openssl commands.
yum -y install openssl nano
- Connect the linux box with existing K8s setup.

Kubectl Installation:
curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl

Step 1: Create a new private key  and CSR

openssl genrsa -out zeal.key 2048
openssl req -new -key zeal.key -out zeal.csr -subj "/CN=zeal/O=kplabs"

Step 2: Encode the csr
cat zeal.csr | base64 | tr -d '\n'

Step 3: Generate the Kubernetes Signing Request 

Use this config format if you are using Kubernetes Version Earlier then 1.22

apiVersion: certificates.k8s.io/v1beta1
kind: CertificateSigningRequest
metadata:
  name: zeal-csr
spec:
  groups:
  - system:authenticated
  request: 
  usages:
  - digital signature
  - key encipherment
  - client auth

Use this config format if you are using Kubernetes Version Above 1.22

apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: zeal-csr
spec:
  signerName: kubernetes.io/kube-apiserver-client
  groups:
  - system:authenticated
  request: 
  usages:
  - digital signature
  - key encipherment
  - client auth

Step 4: Apply the Signing Requests:

kubectl apply -f signingrequest.yaml

Step 5: Approve the csr

kubectl certificate approve zeal-csr

Step 6: Download the Certificate from csr

kubectl get csr zeal-csr -o jsonpath='{.status.certificate}' | base64 -d > zeal.crt

Step 7: Create a new context

kubectl config set-credentials zeal --client-certificate=zeal.crt --client-key=zeal.key

Step 8: Set new Context

kubectl config set-context zeal-context --cluster do-blr1-kplabs-k8s --user=zeal

Step 9: Use Context to Verify

kubectl --context=zeal-context get pods

```

![image](https://user-images.githubusercontent.com/53966749/200822369-d1b1b1af-1c7e-4b0b-b392-5473e141eab2.png)

Understanding Authorization
==========================

![image](https://user-images.githubusercontent.com/53966749/200823226-4e43c370-112c-40a7-bc26-b6e4424ebc58.png)
![image](https://user-images.githubusercontent.com/53966749/200823431-27dd279e-3c1f-44ee-8dcb-f60805671f3d.png)
![image](https://user-images.githubusercontent.com/53966749/200823905-dc907681-1ca6-4e41-ae9d-d0d08d5fd028.png)
![image](https://user-images.githubusercontent.com/53966749/200824067-1305c314-5a98-4927-ac2e-436e1f25fe36.png)
```
role.yaml
------------
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list"]


rolebinding.yaml
----------------
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: role-binding
  namespace: default
subjects:
- kind: User
  name: system:serviceaccount:default:external
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
 ``` 
 ![image](https://user-images.githubusercontent.com/53966749/200824934-0f22f6bc-d5be-4f4e-9805-30b0010131c7.png)
 ![image](https://user-images.githubusercontent.com/53966749/200825018-26ee0807-fe96-4151-b315-d9579837d2d0.png)
![image](https://user-images.githubusercontent.com/53966749/200825101-d7056e9c-14b7-4cec-9cd4-fd70eb4f9c13.png)
![image](https://user-images.githubusercontent.com/53966749/200825157-540de927-792a-43c9-b408-aee7016aebce.png)
![image](https://user-images.githubusercontent.com/53966749/200825225-a6f34e3c-8594-4be6-9143-fb39ba948efb.png)
![image](https://user-images.githubusercontent.com/53966749/200825269-5941d0fc-a43c-4a72-aa29-7b92ca362958.png)

API Groups, Resources and Verbs
===============================
![image](https://user-images.githubusercontent.com/53966749/200826047-5fa44f99-2028-4749-8743-9dba9c9455be.png)
![image](https://user-images.githubusercontent.com/53966749/200827697-484fb5c9-0acf-4405-b41d-c6c22a0efc50.png)

![image](https://user-images.githubusercontent.com/53966749/200827135-b87abc2b-a9f0-4a34-a686-30105305656a.png)
![image](https://user-images.githubusercontent.com/53966749/200828010-80c4f550-fad9-4920-b5b3-cb25c610c4ac.png)

![image](https://user-images.githubusercontent.com/53966749/200827777-d89af18f-9735-48c1-93c5-535d3dd45669.png)

if we dont specify any group by default it will take all, now add create so that the user can create pod
![image](https://user-images.githubusercontent.com/53966749/200828342-dd73a628-f8d7-4883-86ea-fb5d59a7c955.png)

![image](https://user-images.githubusercontent.com/53966749/200828666-0b4c9b4c-c1f4-417c-9c30-c155694e9926.png)


Access Control based on Namespace
================================= 
![image](https://user-images.githubusercontent.com/53966749/200840881-1d46bade-cc89-4cb1-ae16-ff2a4f83296c.png)
![image](https://user-images.githubusercontent.com/53966749/200840958-58b3dde5-7542-45fc-8398-f0bde80e60cc.png)
![image](https://user-images.githubusercontent.com/53966749/200843024-f6b4fe8f-a441-4df3-b860-e853bda6a236.png)

![image](https://user-images.githubusercontent.com/53966749/200843222-68f17616-beb6-4840-8faf-662cab9b3a16.png)
![image](https://user-images.githubusercontent.com/53966749/200843333-38be9296-a447-43cd-b988-e7d5ba7b0679.png)

![image](https://user-images.githubusercontent.com/53966749/200843891-f04359dc-a1c6-4e51-bb70-2b133a1df1fe.png)

![image](https://user-images.githubusercontent.com/53966749/200844283-f503cb17-168b-484a-a760-07e7f769b2e9.png)
![image](https://user-images.githubusercontent.com/53966749/200844355-aa07c7fb-d234-4303-b415-a21e2d98dd13.png)

![image](https://user-images.githubusercontent.com/53966749/200844667-4ec5e619-61eb-4119-aa24-3c6499b57a1e.png)
![image](https://user-images.githubusercontent.com/53966749/200844727-e67928e5-0de0-48d4-b131-235614a93eef.png)

```
teama-role.yaml
----------------
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: teama
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list", "create"]
teama-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: teama
subjects:
- kind: User
  name: zeal
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
  
teama-rolebinding.yaml
----------------------
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: teama
subjects:
- kind: User
  name: zeal
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
  
```

![image](https://user-images.githubusercontent.com/53966749/200841998-f1f0b48e-6b36-40ef-9744-a2aa56ebbb08.png)
![image](https://user-images.githubusercontent.com/53966749/200845841-3e191ac4-4074-42cf-9050-fad5fa18a2f3.png)


Clusterrole and clusterrole binding
====================================
![image](https://user-images.githubusercontent.com/53966749/200847442-5bb99b5e-f037-490e-ace1-2eeeecce3e85.png)

![image](https://user-images.githubusercontent.com/53966749/200848008-5337a08b-222f-4cd7-ac1b-875fb2816b01.png)
![image](https://user-images.githubusercontent.com/53966749/200848065-3be20da6-96da-48fe-b096-00f6dac3874c.png)
```
cat clusterrole.yaml
--------------------
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: [list"]
  
cat clusterrolebinding.yaml
---------------------------
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: list-pods-global
subjects:
- kind: User
  name: zeal
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-pod-reader
  apiGroup: rbac.authorization.k8s.io
  
 ```
  
![image](https://user-images.githubusercontent.com/53966749/200982039-a3d0cd84-021c-4fa2-a8cf-7f2a3bfbc264.png)
![image](https://user-images.githubusercontent.com/53966749/200982135-c00a3cc4-35fd-4590-8dc8-728ac6f308e3.png)

![image](https://user-images.githubusercontent.com/53966749/200982383-e7382c46-9bfd-43d0-bfd4-135dfc36d704.png)

Cluster role can work with also role binding
---------------------------------------------
```
cat cluster-to-rolebind.yaml
------------------------------
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: zeal
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-pod-reader
  apiGroup: rbac.authorization.k8s.io
  
  ```
  ![image](https://user-images.githubusercontent.com/53966749/200982902-473220e2-34cb-4005-a195-3b8d2969e1d7.png)
  ![image](https://user-images.githubusercontent.com/53966749/200983247-b5071bff-cfd0-4d9c-ba0a-1a65298dc8f9.png)
![image](https://user-images.githubusercontent.com/53966749/200983464-70b2a21f-fe71-4f37-ad80-d1fd579aa832.png)


Network Security Policies 
========================
![image](https://user-images.githubusercontent.com/53966749/200983937-8c0f8d24-bbac-4ab8-be0c-13dfb11762c8.png)
![image](https://user-images.githubusercontent.com/53966749/200984035-22e5a46a-8949-4214-a2bb-4bdc0cd07834.png)


bydefault no restriction between pod connectivity
```

root@ubuntu-1:~# kubectl run web --image=nginx --labels="app=web"
pod/web created
root@ubuntu-1:~# kubectl get pod -o wide
NAME   READY   STATUS    RESTARTS   AGE   IP               NODE       NOMINATED NODE   READINESS GATES
web    1/1     Running   0          4s    192.168.243.25   ubuntu-2   <none>           <none>
root@ubuntu-1:~# kubectl run --rm -i -t --image=alpine test-$RANDOM -- sh
If you don't see a command prompt, try pressing enter.
/ # wget 192.168.243.25
Connecting to 192.168.243.25 (192.168.243.25:80)
saving to 'index.html'
index.html           100% |*********************************************************************************************************************************************************|   615  0:00:00 ETA
'index.html' saved
/ #
/ # exit


root@ubuntu-1:~# cat ingress.yaml
------------------------------------
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-pod
  namespace: default
spec:
  podSelector:
    matchLabels:
        app: web
  policyTypes:
  - Ingress

root@ubuntu-1:~# kubectl apply -f ingress.yaml
networkpolicy.networking.k8s.io/default-deny-pod created
root@ubuntu-1:~#
root@ubuntu-1:~# kubectl run --rm -i -t --image=alpine test-$RANDOM -- sh
If you don't see a command prompt, try pressing enter.
/ # wget 192.168.243.25
Connecting to 192.168.243.25 (192.168.243.25:80)
^C
/ # exit


Note: we can use Egress to block all outgoing traffic
 policyTypes:
  - Ingress
  - Egress
  
 Also both Ingress and Egress will work only incase of CNI is installed.
 ```

Kubeconfig
==========
![image](https://user-images.githubusercontent.com/53966749/200995067-72a60545-102a-48b5-9065-03d55022a774.png)
![image](https://user-images.githubusercontent.com/53966749/200995148-db048936-4937-4901-8446-7709932e5451.png)
![image](https://user-images.githubusercontent.com/53966749/200995541-59d52fa0-7afa-42c3-be84-942ce8b08842.png)

![image](https://user-images.githubusercontent.com/53966749/200995957-e2b920d9-fc82-4aef-920a-beb7d138b581.png)
![image](https://user-images.githubusercontent.com/53966749/200996163-4a13d55b-7434-4b58-a5f9-62ede2db5bd8.png)
![image](https://user-images.githubusercontent.com/53966749/200996290-8b28e909-d750-4b23-9687-41a6227c54f6.png)
![image](https://user-images.githubusercontent.com/53966749/200996553-77744e34-7f18-4102-8879-5512ef4483a8.png)


explicitily mention namespace in kubeconfig file

![image](https://user-images.githubusercontent.com/53966749/200996963-9d116a38-2648-4429-b1aa-01e96aef89c0.png)

Access to multiple cluster
--------------------------
![image](https://user-images.githubusercontent.com/53966749/201239072-e9535300-ae8b-4196-b797-346e3e7494a2.png)

```
create kubectl config
-------------------------
1. Add cluster details.
sles15sp3:~ #  kubectl config --kubeconfig=base-config set-cluster ubuntu-k8s --server=https://10.103.16.208
Cluster "ubuntu-k8s" set.

sles15sp3:~ # cat base-config
apiVersion: v1
clusters:
- cluster:
    server: https://10.103.16.208
  name: ubuntu-k8s
contexts: null
current-context: ""
kind: Config
preferences: {}
users: null

add cluster details
-------------------
2. Add user details

kubectl config --kubeconfig=base-config set-credentials experimenter --username=dev --password=some-password
User "experimenter" set.

sles15sp3:~ # cat base-config
apiVersion: v1
clusters:
- cluster:
    server: https://10.103.16.208
  name: ubuntu-k8s
contexts: null
current-context: ""
kind: Config
preferences: {}
users:
- name: experimenter
  user:
    password: some-password
    username: dev

setting context
-----------------
3. Setting Contexts. Namespace is optional , if will specify namespace over here no need
to mention  during running the k8s cmd.

kubectl config --kubeconfig=base-config set-context dev-frontend --cluster=ubuntu-k8s --namespace=frontend --user=experimenter

sles15sp3:~ # kubectl config --kubeconfig=base-config set-context dev-frontend --cluster=ubuntu-k8s --namespace=frontend --user=experimenter
Context "dev-frontend" created.
sles15sp3:~ # cat base-config
apiVersion: v1
clusters:
- cluster:
    server: https://10.103.16.208
  name: ubuntu-k8s
contexts:
- context:
    cluster: ubuntu-k8s
    namespace: frontend
    user: experimenter
  name: dev-frontend
current-context: ""
kind: Config
preferences: {}
users:
- name: experimenter
  user:
    password: some-password
    username: dev

repeat above step for adding 2nd cluster
---------------------------------------
4. Repeating above steps for second cluster

sles15sp3:~ # kubectl config --kubeconfig=base-config set-cluster starlord-k8s --server=https://1.2.3.4
Cluster "starlord-k8s" set.
sles15sp3:~ # kubectl config --kubeconfig=base-config set-context test-frontend --cluster=starlord-k8s  --namespace=frontend --user=experimenter
Context "test-frontend" created.



View Kubeconfig
---------------------

sles15sp3:~ # kubectl config --kubeconfig=base-config view
apiVersion: v1
clusters:
- cluster:
    server: https://1.2.3.4
  name: starlord-k8s
- cluster:
    server: https://10.103.16.208
  name: ubuntu-k8s
contexts:
- context:
    cluster: ubuntu-k8s
    namespace: frontend
    user: experimenter
  name: dev-frontend
- context:
    cluster: starlord-k8s
    namespace: frontend
    user: experimenter
  name: test-frontend
current-context: ""
kind: Config
preferences: {}
users:
- name: experimenter
  user:
    password: some-password
    username: dev
    
Get current conext information
---------------------------------
sles15sp3:~ # kubectl config --kubeconfig=base-config get-contexts
CURRENT   NAME            CLUSTER        AUTHINFO       NAMESPACE
          dev-frontend    ubuntu-k8s     experimenter   frontend
          test-frontend   starlord-k8s   experimenter   frontend

Switch Conexts:
--------------
sles15sp3:~ # kubectl config --kubeconfig=base-config get-contexts
CURRENT   NAME            CLUSTER        AUTHINFO       NAMESPACE
          dev-frontend    ubuntu-k8s     experimenter   frontend
*         test-frontend   starlord-k8s   experimenter   frontend

also we can use runtime context without switching
---------------------------------------------------
sles15sp3:~ #  kubectl config --kubeconfig=base-config  --context=test-frontend get pods

```
