
Labels and selector
====================
![image](https://user-images.githubusercontent.com/53966749/200168456-039f4932-15f0-4444-a216-30df008235f1.png)
![image](https://user-images.githubusercontent.com/53966749/200168478-c5091e0f-d30c-4e61-a9d3-8892243b9405.png)
![image](https://user-images.githubusercontent.com/53966749/200168573-32a1f5a3-7f4d-4a29-9206-3a89f1d05a21.png)

```
sles15sp3:~ # kubectl run nginx --image=nginx
pod/nginx created

sles15sp3:~ # kubectl run nginx2 --image=nginx
pod/nginx2 created

sles15sp3:~ # kubectl get pods
NAME     READY   STATUS    RESTARTS   AGE
nginx    1/1     Running   0          4m39s
nginx2   1/1     Running   0          3s


sles15sp3:~ # kubectl label pods nginx env=dev
pod/nginx labeled

sles15sp3:~ # kubectl label pods nginx2 env=prod
pod/nginx2 labeled

sles15sp3:~ # kubectl get pods --show-labels
NAME     READY   STATUS    RESTARTS   AGE     LABELS
nginx    1/1     Running   0          5m57s   env=dev,run=nginx
nginx2   1/1     Running   0          81s     env=prod,run=nginx2

sles15sp3:~ # kubectl get pods -l env=dev
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          6m24s

sles15sp3:~ # kubectl get pods -l env=prod
NAME     READY   STATUS    RESTARTS   AGE
nginx2   1/1     Running   0          118s
sles15sp3:~ #

sles15sp3:~ # kubectl get pods -l env!=prod
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          6m50s
sles15sp3:~ #

sles15sp3:~ # cat 1.yaml
------------------------
apiVersion: v1
kind: Pod
metadata:
     name: nginxwebserver
     labels:
       env: prod
       app: nginx
spec:
  containers:
     -    image: nginx
          name: democontainer
          ports:
            - containerPort: 8080
sles15sp3:~ # kubectl apply -f 1.yaml
pod/nginxwebserver created
sles15sp3:~ #
sles15sp3:~ # kubectl get pods --show-labels
NAME             READY   STATUS    RESTARTS   AGE   LABELS
nginxwebserver   1/1     Running   0          11s   app=nginx,env=prod

```
Replicaset
=========
![image](https://user-images.githubusercontent.com/53966749/200169525-c1ce631f-7f31-4dce-b2f2-c7ed7c82b7e2.png)
![image](https://user-images.githubusercontent.com/53966749/200169560-6e2db183-981b-45e5-926e-01f9b1ffaf79.png)
```
sles15sp3:~ # cat replicaset.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: kplabs-replicaset
spec:
  replicas: 5
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: nginx
---------------------------------------------------------------
Note: Here  template section similar to pod create section     |
---------------------------------------------------------------

sles15sp3:~ #  kubectl apply -f replicaset.yaml
replicaset.apps/kplabs-replicaset created
sles15sp3:~ #
sles15sp3:~ # kubectl get replicasets.apps
NAME                DESIRED   CURRENT   READY   AGE
kplabs-replicaset   5         5         5       17s
sles15sp3:~ # kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
kplabs-replicaset-5krh8   1/1     Running   0          38s
kplabs-replicaset-ncfrs   1/1     Running   0          38s
kplabs-replicaset-rdhbl   1/1     Running   0          38s
kplabs-replicaset-txqtr   1/1     Running   0          38s
kplabs-replicaset-w8926   1/1     Running   0          38s
nginxwebserver            1/1     Running   0          8m31s
sles15sp3:~ # kubectl delete pod kplabs-replicaset-5krh8
pod "kplabs-replicaset-5krh8" deleted
sles15sp3:~ # kubectl get replicasets.apps
NAME                DESIRED   CURRENT   READY   AGE
kplabs-replicaset   5         5         5       66s
sles15sp3:~ # kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
kplabs-replicaset-ncfrs   1/1     Running   0          71s
kplabs-replicaset-rdhbl   1/1     Running   0          71s
kplabs-replicaset-txqtr   1/1     Running   0          71s
kplabs-replicaset-w8926   1/1     Running   0          71s
kplabs-replicaset-xrvdt   1/1     Running   0          10s
nginxwebserver            1/1     Running   0          9m4s

sles15sp3:~ # kubectl delete replicasets.apps kplabs-replicaset
replicaset.apps "kplabs-replicaset" deleted
sles15sp3:~ # kubectl get pods


delete repicaset without deleting pods
---------------------------------------

sles15sp3:~ # kubectl get replicasets.apps
NAME                DESIRED   CURRENT   READY   AGE
kplabs-replicaset   5         5         5       15s

sles15sp3:~ # kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
kplabs-replicaset-fpbx5   1/1     Running   0          27s
kplabs-replicaset-ssx9x   1/1     Running   0          27s
kplabs-replicaset-sv274   1/1     Running   0          27s
kplabs-replicaset-wttqw   1/1     Running   0          27s
kplabs-replicaset-xn4w4   1/1     Running   0          27s

sles15sp3:~ # kubectl delete replicasets.apps kplabs-replicaset --cascade=false
warning: --cascade=false is deprecated (boolean value) and can be replaced with --cascade=orphan.
replicaset.apps "kplabs-replicaset" deleted

sles15sp3:~ # kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
kplabs-replicaset-fpbx5   1/1     Running   0          71s
kplabs-replicaset-ssx9x   1/1     Running   0          71s
kplabs-replicaset-sv274   1/1     Running   0          71s
kplabs-replicaset-wttqw   1/1     Running   0          71s
kplabs-replicaset-xn4w4   1/1     Running   0          71s

```
Deployments
===========
![image](https://user-images.githubusercontent.com/53966749/200170460-df8b3f85-2341-45f5-a0cc-3ec009b3617f.png)
![image](https://user-images.githubusercontent.com/53966749/200170585-6effef41-21fb-4df6-b7aa-15f75999211f.png)
![image](https://user-images.githubusercontent.com/53966749/200170616-34e35ef3-898f-4bbd-b23b-1a338a933bae.png)

```
simple deployment
------------------

sles15sp3:~ # cat deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kplabs-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: nginx

sles15sp3:~ # kubectl apply -f deployments.yaml
deployment.apps/kplabs-deployment created
sles15sp3:~ #
sles15sp3:~ # kubectl get deployments.apps
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
kplabs-deployment   5/5     5            5           27s
sles15sp3:~ # kubectl get replicasets.apps
NAME                           DESIRED   CURRENT   READY   AGE
kplabs-deployment-6657dc9f69   5         5         5       52s
sles15sp3:~ #
sles15sp3:~ # kubectl get pods
NAME                                 READY   STATUS    RESTARTS   AGE
kplabs-deployment-6657dc9f69-cgpnd   1/1     Running   0          70s
kplabs-deployment-6657dc9f69-ctdkd   1/1     Running   0          70s
kplabs-deployment-6657dc9f69-f99wd   1/1     Running   0          70s
kplabs-deployment-6657dc9f69-jckr5   1/1     Running   0          70s
kplabs-deployment-6657dc9f69-x5gmg   1/1     Running   0          70s

```

rollout changes
--------------
```
sles15sp3:~ # cat deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kplabs-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: nginx:1.17.3

sles15sp3:~ # kubectl apply -f deployments.yaml
deployment.apps/kplabs-deployment configured
sles15sp3:~ # kubectl get replicasets.apps
NAME                           DESIRED   CURRENT   READY   AGE
kplabs-deployment-6657dc9f69   0         0         0       4m54s
kplabs-deployment-7d958b95f    5         5         5       24s

sles15sp3:~ # kubectl describe deployments.apps kplabs-deployment
Name:                   kplabs-deployment
Namespace:              default
CreationTimestamp:      Sun, 06 Nov 2022 12:47:53 +0000
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               tier=frontend
Replicas:               5 desired | 5 updated | 5 total | 5 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  tier=frontend
  Containers:
   php-redis:
    Image:        nginx:1.17.3
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kplabs-deployment-7d958b95f (5/5 replicas created)
Events:
  Type    Reason             Age        From                   Message
  ----    ------             ----       ----                   -------
  Normal  ScalingReplicaSet  3m31s      deployment-controller  Scaled up replica set kplabs-deployment-6657dc9f69 to 5
  Normal  ScalingReplicaSet  <invalid>  deployment-controller  Scaled up replica set kplabs-deployment-7d958b95f to 2
  Normal  ScalingReplicaSet  <invalid>  deployment-controller  Scaled down replica set kplabs-deployment-6657dc9f69 to 4
  Normal  ScalingReplicaSet  <invalid>  deployment-controller  Scaled up replica set kplabs-deployment-7d958b95f to 3
  Normal  ScalingReplicaSet  <invalid>  deployment-controller  Scaled down replica set kplabs-deployment-6657dc9f69 to 3
  Normal  ScalingReplicaSet  <invalid>  deployment-controller  Scaled up replica set kplabs-deployment-7d958b95f to 4
  Normal  ScalingReplicaSet  <invalid>  deployment-controller  Scaled down replica set kplabs-deployment-6657dc9f69 to 2
  Normal  ScalingReplicaSet  <invalid>  deployment-controller  Scaled up replica set kplabs-deployment-7d958b95f to 5
  Normal  ScalingReplicaSet  <invalid>  deployment-controller  Scaled down replica set kplabs-deployment-6657dc9f69 to 1
  Normal  ScalingReplicaSet  <invalid>  deployment-controller  (combined from similar events): Scaled down replica set kplabs-deployment-6657dc9f69 to 0


sles15sp3:~ # kubectl rollout history deployment kplabs-deployment
deployment.apps/kplabs-deployment
REVISION  CHANGE-CAUSE
1         <none>
2         <none>

sles15sp3:~ # kubectl rollout history deployment kplabs-deployment --revision 1
deployment.apps/kplabs-deployment with revision #1
Pod Template:
  Labels:       pod-template-hash=6657dc9f69
        tier=frontend
  Containers:
   php-redis:
    Image:      nginx
    Port:       <none>
    Host Port:  <none>
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>

sles15sp3:~ # kubectl rollout history deployment kplabs-deployment --revision 2
deployment.apps/kplabs-deployment with revision #2
Pod Template:
  Labels:       pod-template-hash=7d958b95f
        tier=frontend
  Containers:
   php-redis:
    Image:      nginx:1.17.3
    Port:       <none>
    Host Port:  <none>
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>
```
![image](https://user-images.githubusercontent.com/53966749/200172070-dd04ffa0-37ea-406b-8e31-5c71aeed7f50.png)


Rolling back deployments
------------------------
```
sles15sp3:~ # kubectl rollout undo  deployment kplabs-deployment --to-revision=1
deployment.apps/kplabs-deployment rolled back

sles15sp3:~ # kubectl get rs
NAME                           DESIRED   CURRENT   READY   AGE
kplabs-deployment-6657dc9f69   5         5         5       13m
kplabs-deployment-7d958b95f    0         0         0       8m47s
sles15sp3:~ #
sles15sp3:~ # kubectl describe deployments.apps kplabs-deployment  | grep image
sles15sp3:~ # kubectl describe deployments.apps kplabs-deployment  | grep -i image
    Image:        nginx
    
sles15sp3:~ # kubectl rollout history deployment kplabs-deployment
deployment.apps/kplabs-deployment
REVISION  CHANGE-CAUSE
2         <none>
3         <none>

```
 Imports deployments commands
 -----------------------------
     
![image](https://user-images.githubusercontent.com/53966749/200173262-8022f27c-0d4c-432c-9a56-226ceb0443b0.png)

```
sles15sp3:~ # kubectl create deployment nginx  --image=nginx:1.9.1  --port=80
deployment.apps/nginx created

sles15sp3:~ # kubectl  get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-5bfdf46dc6-ktdhd   1/1     Running   0          52s
sles15sp3:~ # kubectl describe pod nginx-5bfdf46dc6-ktdhd  | grep -i image
    Image:          nginx:1.9.1

sles15sp3:~ # kubectl set image deployment nginx nginx=nginx --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/nginx image updated

sles15sp3:~ # kubectl rollout history deployment nginx
deployment.apps/nginx
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment nginx nginx=nginx --record=true

sles15sp3:~ # kubectl set image deployment nginx nginx=nginx:1.9.2 --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/nginx image updated

sles15sp3:~ # kubectl describe pod  nginx-7fd9fd6d8f-2pcwv | grep -i image
    Image:          nginx:1.9.2

sles15sp3:~ # kubectl rollout history deployment nginx
deployment.apps/nginx
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment nginx nginx=nginx --record=true
3         kubectl set image deployment nginx nginx=nginx:1.9.2 --record=true

sles15sp3:~ # kubectl rollout undo deployment nginx
deployment.apps/nginx rolled back
sles15sp3:~ #
sles15sp3:~ # kubectl rollout history deployment nginx
deployment.apps/nginx
REVISION  CHANGE-CAUSE
1         <none>
3         kubectl set image deployment nginx nginx=nginx:1.9.2 --record=true
4         kubectl set image deployment nginx nginx=nginx --record=true


scale deployment
-----------------
sles15sp3:~ # kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-66857ff745-dwfzv   1/1     Running   0          4m7s
sles15sp3:~ #

sles15sp3:~ # kubectl scale deployment nginx --replicas=3
deployment.apps/nginx scaled
sles15sp3:~ # kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-66857ff745-dwfzv   1/1     Running   0          4m30s
nginx-66857ff745-fljn5   1/1     Running   0          3s
nginx-66857ff745-h8ptg   1/1     Running   0          3s


```
Generate deployment manifest
-----------------------------
```
sles15sp3:~ # kubectl create deployment nginx-web  --image=nginx --port=80 --dry-run=client -o yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx-web
  name: nginx-web
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-web
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx-web
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
        resources: {}
status: {}

```
![image](https://user-images.githubusercontent.com/53966749/200174830-e14ab95e-d0a4-4501-a4f3-d18f16fa876c.png)

Deamonset
=========

![image](https://user-images.githubusercontent.com/53966749/200175959-84133b1f-a3d0-40df-9197-386bd4bde589.png)

```
sles15sp3:~ # cat deamonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kplabs-daemonset
spec:
  selector:
    matchLabels:
      name: kplabs-all-pods
  template:
    metadata:
      labels:
        name: kplabs-all-pods
    spec:
      containers:
      - name: kplabs-pods
        image: nginx

sles15sp3:~ # kubectl get nodes
NAME                                       STATUS   ROLES    AGE   VERSION
gke-cluster-1-default-pool-37115ec3-0z9n   Ready    <none>   9h    v1.22.12-gke.2300
gke-cluster-1-default-pool-37115ec3-9hs1   Ready    <none>   9h    v1.22.12-gke.2300
gke-cluster-1-default-pool-37115ec3-bdw9   Ready    <none>   9h    v1.22.12-gke.2300

sles15sp3:~ # kubectl apply -f deamonset.yaml
daemonset.apps/kplabs-daemonset created

sles15sp3:~ # kubectl get pods -o wide
NAME                     READY   STATUS    RESTARTS   AGE   IP          NODE                                       NOMINATED NODE   READINESS GATES
kplabs-daemonset-49dz8   1/1     Running   0          32s   10.8.2.16   gke-cluster-1-default-pool-37115ec3-9hs1   <none>           <none>
kplabs-daemonset-jzpv5   1/1     Running   0          32s   10.8.0.46   gke-cluster-1-default-pool-37115ec3-0z9n   <none>           <none>
kplabs-daemonset-ppdxn   1/1     Running   0          32s   10.8.1.20   gke-cluster-1-default-pool-37115ec3-bdw9   <none>           <none>

sles15sp3:~ # kubectl get  daemonset
NAME               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
kplabs-daemonset   3         3         3       3            3           <none>          2m11s

```

Nodeselector
============
![image](https://user-images.githubusercontent.com/53966749/200176313-3955fb5b-d498-4c5a-99c5-b204b1fc14b0.png)
![image](https://user-images.githubusercontent.com/53966749/200176331-e90b2d95-1a89-490e-b1ab-0e5978138c9b.png)

![image](https://user-images.githubusercontent.com/53966749/200176375-90e05161-9c2e-4c3b-929f-064fbb226415.png)

![image](https://user-images.githubusercontent.com/53966749/200176812-5c3f5671-5f77-4073-88c4-e2c08e74cc96.png)


```
sles15sp3:~ # kubectl get nodes
NAME                                       STATUS   ROLES    AGE   VERSION
gke-cluster-1-default-pool-37115ec3-0z9n   Ready    <none>   10h   v1.22.12-gke.2300
gke-cluster-1-default-pool-37115ec3-9hs1   Ready    <none>   10h   v1.22.12-gke.2300
gke-cluster-1-default-pool-37115ec3-bdw9   Ready    <none>   10h   v1.22.12-gke.2300
sles15sp3:~ # kubectl label nodes
nodes                 nodes.metrics.k8s.io
sles15sp3:~ # kubectl label nodes
nodes                 nodes.metrics.k8s.io
sles15sp3:~ # kubectl label nodes gke-cluster-1-default-pool-37115ec3-0z9n disk=hdd
node/gke-cluster-1-default-pool-37115ec3-0z9n labeled

sles15sp3:~ # kubectl describe nodes gke-cluster-1-default-pool-37115ec3-0z9n | grep -i disk
                    cloud.google.com/gke-boot-disk=pd-standard
                    disk=hdd

sles15sp3:~ # kubectl label nodes gke-cluster-1-default-pool-37115ec3-9hs1 disk=hdd
node/gke-cluster-1-default-pool-37115ec3-9hs1 labeled
sles15sp3:~ #
sles15sp3:~ # kubectl label nodes gke-cluster-1-default-pool-37115ec3-bdw9  disk=ssd
node/gke-cluster-1-default-pool-37115ec3-bdw9 labeled



-------------
sles15sp3:~ # cat nodeselector.yaml
apiVersion: v1
kind: Pod
metadata:
  name: service-pod
spec:
  containers:
  - name: service-pod
    image: nginx
  nodeSelector:
    disk: ssd

sles15sp3:~ # kubectl apply -f nodeselector.yaml
pod/service-pod created

sles15sp3:~ # kubectl get pods -o wide
NAME          READY   STATUS    RESTARTS   AGE   IP          NODE                                       NOMINATED NODE   READINESS GATES
service-pod   1/1     Running   0          16s   10.8.1.21   gke-cluster-1-default-pool-37115ec3-bdw9   <none>           <none>
sles15sp3:~ #
sles15sp3:~ # kubectl describe nodes gke-cluster-1-default-pool-37115ec3-bdw9 | grep -i disk
                    cloud.google.com/gke-boot-disk=pd-standard
                     disk=ssd

pod to node         
-----------

sles15sp3:~/test # cat podtonodes.yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
  labels:
    app: frontend
spec:
  containers:
  - name: frontend
    image: nginx
  nodeSelector:
    kubernetes.io/hostname: gke-cluster-1-default-pool-37115ec3-beno
sles15sp3:~/test # kubectl get nodes
NAME                                       STATUS   ROLES    AGE     VERSION
gke-cluster-1-default-pool-37115ec3-beno   Ready    <none>   3h35m   v1.23.8-gke.1900
gke-cluster-1-default-pool-37115ec3-csyj   Ready    <none>   3h31m   v1.23.8-gke.1900
gke-cluster-1-default-pool-37115ec3-ngw4   Ready    <none>   3h27m   v1.23.8-gke.1900
sles15sp3:~/test #
sles15sp3:~/test # kubectl apply -f podtonodes.yaml
pod/frontend created
sles15sp3:~/test #
sles15sp3:~/test # kubectl get pods -o wide
NAME       READY   STATUS    RESTARTS   AGE   IP          NODE                                       NOMINATED NODE   READINESS GATES
frontend   1/1     Running   0          13s   10.8.3.14   gke-cluster-1-default-pool-37115ec3-beno   <none>           <none>
sles15sp3:~/test #

```

Node Affinity
=============
![image](https://user-images.githubusercontent.com/53966749/200176972-0f2cee9d-c904-4141-a714-a065d7d5b348.png)
![image](https://user-images.githubusercontent.com/53966749/200176999-7de722b9-ed52-4ac5-830c-a261e8d36b55.png)


![image](https://user-images.githubusercontent.com/53966749/200177054-74f4084d-63ea-4a41-81ee-c3eb86e414b6.png)


Node affinity-required
-----------------------
```
sles15sp3:~ # cat node_aff_req.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disk
            operator: In
            values:
            - ssd
  containers:
  - name: with-node-affinity
    image: nginx

sles15sp3:~ # kubectl apply -f node_aff_req.yaml
pod/kplabs-node-affinity created

sles15sp3:~ # kubectl get pods -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP          NODE                                       NOMINATED NODE   READINESS GATES
kplabs-node-affinity   1/1     Running   0          17s   10.8.1.22   gke-cluster-1-default-pool-37115ec3-bdw9   <none>           <none>
sles15sp3:~ # kubectl describe nodes gke-cluster-1-default-pool-37115ec3-bdw9 | grep -i disk
                    cloud.google.com/gke-boot-disk=pd-standard
                    disk=ssd
NotIn
-----
sles15sp3:~ # cat node_aff_req.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disk
            operator: NotIn
            values:
            - ssd
  containers:
  - name: with-node-affinity
    image: nginx

sles15sp3:~ # kubectl apply -f node_aff_req.yaml
pod/kplabs-node-affinity created

sles15sp3:~ # kubectl get pods -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP          NODE                                       NOMINATED NODE   READINESS GATES
kplabs-node-affinity   1/1     Running   0          22s   10.8.0.47   gke-cluster-1-default-pool-37115ec3-0z9n   <none>           <none>

sles15sp3:~ # kubectl describe nodes gke-cluster-1-default-pool-37115ec3-0z9n  | grep -i disk
                    cloud.google.com/gke-boot-disk=pd-standard
                    disk=hdd

wrong key-value
--------------
sles15sp3:~ # cat node_aff_req.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disk
            operator: In
            values:
            - ssd2
  containers:
  - name: with-node-affinity
    image: nginx

sles15sp3:~ # kubectl apply -f node_aff_req.yaml
pod/kplabs-node-affinity created

sles15sp3:~ # kubectl get pod
NAME                   READY   STATUS    RESTARTS   AGE
kplabs-node-affinity   0/1     Pending   0          15s

sles15sp3:~ # kubectl describe pod kplabs-node-affinity
Events:
  Type     Reason             Age        From                Message
  ----     ------             ----       ----                -------
  Warning  FailedScheduling   <invalid>  default-scheduler   0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.
  Normal   NotTriggerScaleUp  <invalid>  cluster-autoscaler  pod didn't trigger scale-up:
```

Node affinity-prefered
-----------------------
if label are not found still it will run it
```
sles15sp3:~ # cat nodeAffinity-preferred.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-node-affinity-preferred
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: memory
            operator: In
            values:
            - high
            - medium
  containers:
  - name: kplabs-affinity-prefferd
    image: nginx
sles15sp3:~ # kubectl apply -f nodeAffinity-preferred.yaml
pod/kplabs-node-affinity-preferred created
sles15sp3:~ #
sles15sp3:~ # kubectl get pod -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE                                       NOMINATED NODE   READINESS GATES
kplabs-node-affinity-preferred   1/1     Running   0          13s   10.8.0.48   gke-cluster-1-default-pool-37115ec3-0z9n   <none>           <none>
sles15sp3:~ #
```
 ![image](https://user-images.githubusercontent.com/53966749/200178289-9ef34d96-0ae7-4d6f-a0b4-339f15f41b13.png)
 
 

Pod Affinity and Anti-Affinity
================================

![image](https://user-images.githubusercontent.com/53966749/200178407-2b0f576a-026e-4dac-b872-2a5320405824.png)
![image](https://user-images.githubusercontent.com/53966749/200178427-1a2b15f3-9059-46ca-96c0-be645b93e383.png)
![image](https://user-images.githubusercontent.com/53966749/200178471-ff76c3e1-2fda-4cc9-8cfe-104cac883783.png)
![image](https://user-images.githubusercontent.com/53966749/200178510-a8339a77-eec1-4ca0-a43f-148bd7dbbc21.png)
![image](https://user-images.githubusercontent.com/53966749/200178540-ff5c6efc-3641-46f1-bc71-899a8dd1b209.png)

Pod Affinity
-------------
```
sles15sp3:~ #  kubectl run nginx-web --image=nginx
pod/nginx-web created
sles15sp3:~ #

sles15sp3:~ # kubectl get pods -o wide
NAME        READY   STATUS    RESTARTS   AGE     IP          NODE                                       NOMINATED NODE   READINESS GATES
nginx-web   1/1     Running   0          3m22s   10.8.0.52   gke-cluster-1-default-pool-37115ec3-0z9n   <none>           <none>
sles15sp3:~ #


sles15sp3:~ #
sles15sp3:~ # kubectl label pod nginx-web app=frontend
pod/nginx-web labeled

sles15sp3:~ # kubectl describe pod nginx-web | grep -i label
Labels:       app=frontend

----------
sles15sp3:~ # cat podaffinity-required.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - frontend
        topologyKey: kubernetes.io/hostname
  containers:
  - name: pod-affinity
    image: nginx

sles15sp3:~ # kubectl apply -f podaffinity-required.yaml
pod/kplabs-pod-affinity created
sles15sp3:~ #
sles15sp3:~ #  kubectl get pods -o wide
NAME                  READY   STATUS    RESTARTS   AGE     IP          NODE                                       NOMINATED NODE   READINESS GATES
kplabs-pod-affinity   1/1     Running   0          7s      10.8.0.53   gke-cluster-1-default-pool-37115ec3-0z9n   <none>           <none>
nginx-web             1/1     Running   0          3m58s   10.8.0.52   gke-cluster-1-default-pool-37115ec3-0z9n   <none>           <none>
sles15sp3:~ #
```
Pod Anti-Affinity
--------------

```
sles15sp3:~ # cat podanti_affinity-required.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-pod-anti-affinity
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - frontend
        topologyKey: kubernetes.io/hostname
  containers:
  - name: pod-affinity
    image: nginx
sles15sp3:~ # kubectl apply -f podanti_affinity-required.yaml
pod/kplabs-pod-anti-affinity created
sles15sp3:~ # kubectl get pods -o wide --show-labels
NAME                       READY   STATUS    RESTARTS   AGE   IP          NODE                                       NOMINATED NODE   READINESS GATES   LABELS
kplabs-pod-anti-affinity   1/1     Running   0          5s    10.8.1.23   gke-cluster-1-default-pool-37115ec3-bdw9   <none>           <none>            <none>
nginx-web                  1/1     Running   0          20m   10.8.0.52   gke-cluster-1-default-pool-37115ec3-0z9n   <none>           <none>            app=frontend,run=nginx-web
sles15sp3:~ #

```

Resource limits
===============
![image](https://user-images.githubusercontent.com/53966749/200210726-48b6ef8d-602d-4d5e-9c83-50fdad28ee88.png)
![image](https://user-images.githubusercontent.com/53966749/200211461-998e3443-78c5-460a-8edb-d1bd0e97022c.png)

```
sles15sp3:~/test # cat request_limit.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-pod
spec:
  containers:
  - name: kplabs-container
    image: nginx
    resources:
      requests:
        memory: "640Mi"
        cpu: "0.5"
      limits:
        memory: "12800Mi"
        cpu: "1"

sles15sp3:~/test # kubectl apply -f request_limit.yaml
pod/kplabs-pod created

sles15sp3:~/test # kubectl get pods -o wide
NAME         READY   STATUS    RESTARTS   AGE   IP         NODE                                       NOMINATED NODE   READINESS GATES
kplabs-pod   1/1     Running   0          21s   10.8.0.4   gke-cluster-1-default-pool-37115ec3-tzq4   <none>           <none>

sles15sp3:~/test # kubectl describe nodes gke-cluster-1-default-pool-37115ec3-tzq4

  Namespace                   Name                                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits   Age
  ---------                   ----                                                   ------------  ----------  ---------------  -------------   ---
  default                     kplabs-pod                                             500m (53%)    1 (106%)    640Mi (22%)      12800Mi (455%)  5m25s
  kube-system                 fluentbit-gke-27w8b                                    100m (10%)    0 (0%)      200Mi (7%)       500Mi (17%)     27m
  kube-system                 gke-metrics-agent-kr4v7                                8m (0%)       0 (0%)      100Mi (3%)       100Mi (3%)      27m
  kube-system                 kube-dns-autoscaler-fbc66b884-pkkmv                    20m (2%)      0 (0%)      10Mi (0%)        0 (0%)          10h
  kube-system                 kube-proxy-gke-cluster-1-default-pool-37115ec3-tzq4    100m (10%)    0 (0%)      0 (0%)           0 (0%)          27m
  kube-system                 pdcsi-node-sqwtr                                       10m (1%)      0 (0%)      20Mi (0%)        100Mi (3%)      27m
  test                        redis-cache-56cd95799f-gpbr9                           0 (0%)        0 (0%)      0 (0%)           0 (0%)          10h


ex-2  , change the memory size
-------------------------------
sles15sp3:~/test # cat request_limit.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-pod
spec:
  containers:
  - name: kplabs-container
    image: nginx
    resources:
      requests:
        memory: "6400Mi"
        cpu: "0.5"
      limits:
        memory: "12800Mi"
        cpu: "1"

sles15sp3:~/test # kubectl apply -f request_limit.yaml
pod/kplabs-pod created
sles15sp3:~/test # kubectl get pods
NAME         READY   STATUS    RESTARTS   AGE
kplabs-pod   0/1     Pending   0          4s

sles15sp3:~/test # kubectl describe pod kplabs-pod
Events:
  Type     Reason             Age        From                Message
  ----     ------             ----       ----                -------
  Warning  FailedScheduling   <invalid>  default-scheduler   0/3 nodes are available: 1 Insufficient cpu, 3 Insufficient memory.
  Normal   NotTriggerScaleUp  <invalid>  cluster-autoscaler  pod didn't trigger scale-up:


Note- Schduler schdule based on reqest resource availbility thats why eventhough limit resource is more than 
in the node still it will run 

```

Static pods
===========

![image](https://user-images.githubusercontent.com/53966749/200213163-8d1b9242-67a2-4e27-96f3-6635d3e78717.png)
![image](https://user-images.githubusercontent.com/53966749/200215189-adfa07c2-3c42-4f97-8219-35bb733f3b45.png)

```
root@ubuntu-1:/etc/kubernetes/manifests# ll
total 28
drwxr-xr-x 2 root root 4096 Sep 18 16:18 ./
drwxr-xr-x 4 root root 4096 Sep 19 04:44 ../
-rw------- 1 root root 2380 Sep 18 16:18 etcd.yaml
-rw------- 1 root root 4489 Sep 18 16:18 kube-apiserver.yaml
-rw------- 1 root root 3996 Sep 18 16:18 kube-controller-manager.yaml
-rw------- 1 root root 1915 Sep 18 16:18 kube-scheduler.yaml
root@ubuntu-1:/etc/kubernetes/manifests# vi kplabs-pods.yaml

root@ubuntu-1:/etc/kubernetes/manifests# cat kplabs-pods.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-pod
spec:
  containers:
  - name: kplabs-container
    image: nginx



root@ubuntu-1:/etc/kubernetes/manifests# kubectl get pods|  grep kplabs
kplabs-pod-ubuntu-1     1/1     Running     0          54s

root@ubuntu-1:/etc/kubernetes/manifests# rm -f kplabs-pods.yaml
root@ubuntu-1:/etc/kubernetes/manifests#
root@ubuntu-1:/etc/kubernetes/manifests# kubectl get pods|  grep kplabs

```
Taints and tolerance
=====================

![image](https://user-images.githubusercontent.com/53966749/200217729-e26b4480-d37a-4768-98c3-d1c2298a2fe4.png)
![image](https://user-images.githubusercontent.com/53966749/200217831-b907caf3-cdb4-43f7-8b4a-d6cd0124dacb.png)


NoSchedule
-----------
```

sles15sp3:~/test # kubectl get nodes
NAME                                       STATUS   ROLES    AGE   VERSION
gke-cluster-1-default-pool-37115ec3-1p7j   Ready    <none>   89m   v1.22.12-gke.2300
gke-cluster-1-default-pool-37115ec3-gq98   Ready    <none>   89m   v1.22.12-gke.2300
gke-cluster-1-default-pool-37115ec3-tzq4   Ready    <none>   89m   v1.22.12-gke.2300

sles15sp3:~/test # kubectl describe nodes gke-cluster-1-default-pool-37115ec3-1p7j | grep -i taint
                    node.gke.io/last-applied-node-taints:
Taints:             <none>
sles15sp3:~/test #
sles15sp3:~/test # kubectl taint nodes gke-cluster-1-default-pool-37115ec3-1p7j key1=value1:NoSchedule
node/gke-cluster-1-default-pool-37115ec3-1p7j tainted
sles15sp3:~/test #
sles15sp3:~/test # kubectl describe nodes gke-cluster-1-default-pool-37115ec3-1p7j | grep -i taint
                    node.gke.io/last-applied-node-taints:
Taints:             key1=value1:NoSchedule


sles15sp3:~/test # kubectl create deployment my-nginx --image=nginx --replicas=5
deployment.apps/my-nginx created
sles15sp3:~/test # kubectl get pods -o wide
NAME                       READY   STATUS    RESTARTS   AGE   IP         NODE                                       NOMINATED NODE   READINESS GATES
my-nginx-c54945c55-6gtdx   1/1     Running   0          8s    10.8.0.8   gke-cluster-1-default-pool-37115ec3-tzq4   <none>           <none>
my-nginx-c54945c55-99ppb   1/1     Running   0          8s    10.8.0.7   gke-cluster-1-default-pool-37115ec3-tzq4   <none>           <none>
my-nginx-c54945c55-h95lj   1/1     Running   0          8s    10.8.1.5   gke-cluster-1-default-pool-37115ec3-gq98   <none>           <none>
my-nginx-c54945c55-h9qvk   1/1     Running   0          8s    10.8.1.6   gke-cluster-1-default-pool-37115ec3-gq98   <none>           <none>
my-nginx-c54945c55-zvpqv   1/1     Running   0          8s    10.8.0.6   gke-cluster-1-default-pool-37115ec3-tzq4   <none>           <none>

```
if the pod having pass i.e tolerance then it will also run in noschdule node i.e- gke-cluster-1-default-pool-37115ec3-1p7j
also will run on the node doesnt having tailnts

![image](https://user-images.githubusercontent.com/53966749/200219159-bdfd267a-0fff-4519-b687-5df336d81fb6.png)

to remove tailnt just put - at the end
---------------------------------------
```
sles15sp3:~/test # kubectl taint nodes gke-cluster-1-default-pool-37115ec3-1p7j key1=value1:NoSchedule-
node/gke-cluster-1-default-pool-37115ec3-1p7j untainted
sles15sp3:~/test # kubectl describe nodes gke-cluster-1-default-pool-37115ec3-1p7j | grep -i taint
                    node.gke.io/last-applied-node-taints:
Taints:             <none>
```
     
components of taint and tolerance
----------------------------------

 ![image](https://user-images.githubusercontent.com/53966749/200220033-913b76de-bf7b-4eb6-b6c3-aa84af3e8bd6.png)
 ![image](https://user-images.githubusercontent.com/53966749/200220101-1b855d8f-57a5-48c1-8e6f-dc049116d051.png)
![image](https://user-images.githubusercontent.com/53966749/200220256-6015a9e0-c23d-4468-8e4d-c370cca8db11.png)

```
 sles15sp3:~/test # kubectl create deployment my-nginx --image=nginx --replicas=5
deployment.apps/my-nginx created
sles15sp3:~/test # kubectl get pods -o wide
NAME                       READY   STATUS    RESTARTS   AGE   IP          NODE                                       NOMINATED NODE   READINESS GATES
my-nginx-c54945c55-6p4m2   1/1     Running   0          6s    10.8.0.9    gke-cluster-1-default-pool-37115ec3-tzq4   <none>           <none>
my-nginx-c54945c55-8x5gj   1/1     Running   0          6s    10.8.0.10   gke-cluster-1-default-pool-37115ec3-tzq4   <none>           <none>
my-nginx-c54945c55-9z87n   1/1     Running   0          6s    10.8.0.11   gke-cluster-1-default-pool-37115ec3-tzq4   <none>           <none>
my-nginx-c54945c55-jplwc   1/1     Running   0          6s    10.8.1.8    gke-cluster-1-default-pool-37115ec3-gq98   <none>           <none>
my-nginx-c54945c55-vjklt   1/1     Running   0          6s    10.8.1.7    gke-cluster-1-default-pool-37115ec3-gq98   <none>           <none>

     
sles15sp3:~/test # kubectl taint nodes gke-cluster-1-default-pool-37115ec3-tzq4 key1=value1:NoExecute
node/gke-cluster-1-default-pool-37115ec3-tzq4 tainted

sles15sp3:~/test # kubectl get pods -o wide
NAME                       READY   STATUS    RESTARTS   AGE     IP          NODE                                       NOMINATED NODE   READINESS GATES
my-nginx-c54945c55-46zrh   1/1     Running   0          15s     10.8.2.11   gke-cluster-1-default-pool-37115ec3-1p7j   <none>           <none>
my-nginx-c54945c55-jplwc   1/1     Running   0          2m41s   10.8.1.8    gke-cluster-1-default-pool-37115ec3-gq98   <none>           <none>
my-nginx-c54945c55-n2bsw   1/1     Running   0          15s     10.8.1.10   gke-cluster-1-default-pool-37115ec3-gq98   <none>           <none>
my-nginx-c54945c55-sbddr   1/1     Running   0          14s     10.8.2.12   gke-cluster-1-default-pool-37115ec3-1p7j   <none>           <none>
my-nginx-c54945c55-vjklt   1/1     Running   0          2m41s   10.8.1.7    gke-cluster-1-default-pool-37115ec3-gq98   <none>           <none>



taints
 ---------
 Taint effects define what will happen to pods if they don’t tolerate the taints. The three taint effects are:

NoSchedule: A strong effect where the system lets the pods already scheduled in the nodes run, but enforces taints from the subsequent pods.
PreferNoSchedule: A soft effect where the system will try to avoid placing a pod that does not tolerate the taint on the node.
NoExecute: A strong effect where all previously scheduled pods are evicted, and new pods that don’t tolerate the taint will not be scheduled.

kubectl taint nodes node1 key1=value1:NoSchedule
kubectl taint nodes node1 key1=value1:NoExecute
kubectl taint nodes node1 key2=value1:PreferNoSchedule


Taints don’t allow pods to schedule on nodes with the set key-value property, but how will you schedule a pod to these nodes with taints?
That’s where tolerations come in. They help you schedule pods on the nodes with the taints. Tolerations are applied to your pods’ manifests in the following format:

tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
  tolerationSeconds: 360
  
  if the taints and tolerations match, the pods can be scheduled on the tainted nodes, but there’s not a requirement that the pods be scheduled on the tainted nodes. 
  If there’s a node without taints, a pod with tolerations can be scheduled on that node, even if there’s also an available node with tolerable taints.
 
kubernetes, by default, taints your master node with NoSchedule to prevent it from crashing under high loads.
remove the taints, but that’s not recommended in production environments. The command to remove is:
root@ubuntu-1:~# kubectl taint nodes ubuntu-1 key1-
node/ubuntu-1 untainted

```


Multicontainer Pod Pattern
===========================
![image](https://user-images.githubusercontent.com/53966749/200221179-e364a4fc-7218-439e-894e-5d4a656984e8.png)
![image](https://user-images.githubusercontent.com/53966749/200221268-3ad1cd62-ae67-4b76-9398-f5e7ff67b0fd.png)
![image](https://user-images.githubusercontent.com/53966749/200221389-ae9b9036-531e-42e2-8cc5-d7dfa0d7019d.png)


Adaptor Pattern
---------------

```
sles15sp3:~/test # cat adaptor.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
data:
  fluentd.conf: |
    <source>
      type tail
      format none
      path /var/log/1.log
      pos_file /var/log/1.log.pos
      tag PHP
    </source>
    <source>
      type tail
      format none
      path /var/log/2.log
      pos_file /var/log/2.log.pos
      tag JAVA
    </source>
    <match **>
       @type file
       path /var/log/fluent/access
    </match>
---
apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-agent
    image: k8s.gcr.io/fluentd-gcp:1.30
    env:
    - name: FLUENTD_ARGS
      value: -c /etc/fluentd-config/fluentd.conf
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: config-volume
      mountPath: /etc/fluentd-config
  volumes:
  - name: varlog
    emptyDir: {}
  - name: config-volume
    configMap:
      name: fluentd-config

----------------------
sles15sp3:~/test # kubectl exec -it counter -- sh
Defaulted container "count" out of: count, count-agent
/ # cd /var/log/
/var/log # ls
1.log      1.log.pos  2.log      2.log.pos  fluent     journal
/var/log # cat 1.log | head -3
0: Mon Nov  7 04:17:54 UTC 2022
1: Mon Nov  7 04:17:55 UTC 2022
2: Mon Nov  7 04:17:56 UTC 2022

/var/log # cat 2.log | head -3
Mon Nov  7 04:17:54 UTC 2022 INFO 0
Mon Nov  7 04:17:55 UTC 2022 INFO 1
Mon Nov  7 04:17:56 UTC 2022 INFO 2

/var/log # cd fluent
/var/log/fluent # ls
access.20221107.b5ecd9b5f0d413938
/var/log/fluent # head -5 access.20221107.b5ecd9b5f0d413938
2022-11-07T04:18:08+00:00       PHP     {"message":"14: Mon Nov  7 04:18:08 UTC 2022"}
2022-11-07T04:18:08+00:00       JAVA    {"message":"Mon Nov  7 04:18:08 UTC 2022 INFO 14"}
2022-11-07T04:18:09+00:00       PHP     {"message":"15: Mon Nov  7 04:18:09 UTC 2022"}
2022-11-07T04:18:09+00:00       JAVA    {"message":"Mon Nov  7 04:18:09 UTC 2022 INFO 15"}
2022-11-07T04:18:10+00:00       PHP     {"message":"16: Mon Nov  7 04:18:10 UTC 2022"}

```

